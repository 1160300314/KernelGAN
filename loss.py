import torch
import torch.nn as nn
from torch.autograd import Variable
from util import shave_a2b, resize_tensor_w_kernel, create_gaussian, map2tensor
import numpy as np


# noinspection PyUnresolvedReferences
class GANLoss(nn.Module):
    """ Receiving the final layer from the discriminator and a boolean indicating whether the input to the
     discriminator is real or fake (generated by generator), this returns a patch"""
    def __init__(self, d_last_layer_size):
        super(GANLoss, self).__init__()
        # Note: When activated as a loss between to feature-maps, then a loss-map is created. However, using defaults
        # for MSEloss, this map is averaged and reduced to a single scalar
        self.loss = nn.L1Loss(reduction='mean')

        # The two possible label maps are pre-prepared
        self.label_tensor_fake = Variable(torch.zeros(d_last_layer_size).cuda(), requires_grad=False)
        self.label_tensor_real = Variable(torch.ones(d_last_layer_size).cuda(), requires_grad=False)

    def forward(self, d_last_layer, is_d_input_real, grad_map):
        # Determine label map according to whether current input to discriminator is real or fake
        label_tensor = self.label_tensor_real if is_d_input_real else self.label_tensor_fake
        # Finally compute the loss
        return self.loss(d_last_layer, label_tensor)


class DownScaleLoss(nn.Module):
    """ Computes the difference between the Generator's downscaling and a bench mark downscaling e.g. bicubic, should return a tensor"""
    def __init__(self, kernel, scale_factor):
        super(DownScaleLoss, self).__init__()
        self.loss = nn.MSELoss()
        self.kernel = Variable(torch.Tensor(kernel).cuda(), requires_grad=False)
        self.scale_factor = scale_factor

    def forward(self, g_input, g_output):
        downscaled = resize_tensor_w_kernel(im_t=g_input, k=self.kernel, sf=self.scale_factor)

        # Shave the downscaled to fit g_output
        return self.loss(g_output, shave_a2b(downscaled, g_output))


class SumOfWeightsLoss(nn.Module):
    """ Computes the difference between the Generator's downscaling and a bench mark downscaling e.g. bicubic, should return a tensor"""
    def __init__(self):
        super(SumOfWeightsLoss, self).__init__()
        self.loss = nn.L1Loss()

    def forward(self, kernel):
        return self.loss(1., torch.sum(kernel))


class CentralizedLoss(nn.Module):
    """ Penalizes distance of center of mass from K's center"""
    def __init__(self, k_size, power, scale_factor=.5, func='COM'):
        super(CentralizedLoss, self).__init__()
        self.k_size = k_size
        self.power = power
        self.indices = Variable(torch.arange(0., float(k_size)).cuda(), requires_grad=False)
        wanted_center_of_mass = k_size // 2 + 0.5 * (int(1/scale_factor) - k_size % 2)
        self.center = Variable(torch.FloatTensor([wanted_center_of_mass, wanted_center_of_mass]).cuda(), requires_grad=False)
        self.loss = nn.MSELoss()
        self.penalty_function = func

    def center_of_mass(self, kernel):
        """Given a kernel, calculates it's center of mass and returns it's distance from the kernel's center"""
        r_sum, c_sum = torch.sum(kernel, dim=1).reshape(1, -1), torch.sum(kernel, dim=0).reshape(1, -1)
        return self.loss(torch.stack((torch.matmul(r_sum, self.indices) / torch.sum(kernel), torch.matmul(c_sum, self.indices) / torch.sum(kernel))), self.center)

    def asymmetry_measure(self, kernel):
        """Given a kernel calculates the sum on each side of the center"""
        kernel = self.raise_power_keep_sign(kernel)
        left_sum, right_sum = torch.sum(kernel[:, 0:self.k_size//2]), torch.sum(kernel[:, self.k_size//2 + self.k_size % 2:])
        top_sum, bot_sum = torch.sum(kernel[0:self.k_size//2, :]), torch.sum(kernel[self.k_size//2 + self.k_size % 2:, :])
        return torch.abs(left_sum - right_sum) + torch.abs(top_sum - bot_sum)

    def raise_power_keep_sign(self, kernel):
        """Raises a kernel to the power of self.power keeping the sign of each entry in the kernel"""
        return kernel ** self.power if self.power % 2 == 1 else torch.abs(kernel) * (kernel ** (self.power - 1))

    def forward(self, kernel):
        """Return the loss over the distance of center of mass from kernel center """
        return self.asymmetry_measure(kernel) if self.penalty_function == 'SYM' else self.center_of_mass(kernel)


class MaxCenter(nn.Module):
    """ Penalizes difference of max value from the value of K's center"""
    def __init__(self, k_size):
        super(MaxCenter, self).__init__()
        self.k_size = k_size
        self.loss = nn.MSELoss()

    def forward(self, kernel):
        """Return the difference of the center pixel from the max pixel"""
        return self.loss(kernel[self.k_size // 2, self.k_size // 2], torch.max(kernel))


class SparseEdgesLoss(nn.Module):
    """ Gives a higher loss the farther from center you are """
    def __init__(self, k_size):
        super(SparseEdgesLoss, self).__init__()
        self.k_size = k_size
        self.mask = map2tensor(self.create_mask(penalty_scale=30))
        self.zero_label = Variable(torch.zeros(k_size).cuda(), requires_grad=False)
        self.loss = nn.L1Loss()

    def create_mask(self, penalty_scale):
        center_size = self.k_size // 2 + self.k_size % 2
        mask = create_gaussian(size=self.k_size, sigma1=self.k_size, is_tensor=False)
        mask = 1 - mask / np.max(mask)
        margin = (self.k_size - center_size) // 2 - 1
        mask[margin:-margin, margin:-margin] = 0
        return penalty_scale * mask

    def forward(self, kernel):
        return self.loss(kernel * self.mask, self.zero_label)


class SparsityLoss(nn.Module):
    """ Gives a higher loss the farther from center you are """
    def __init__(self):
        super(SparsityLoss, self).__init__()
        self.power = 0.2
        self.loss = nn.L1Loss()

    def forward(self, kernel):
        return self.loss(torch.abs(kernel) ** self.power, torch.zeros_like(kernel))


class NegativeValuesLoss(nn.Module):
    """Penalizes negative values"""
    def __init__(self):
        super(NegativeValuesLoss, self).__init__()

    def forward(self, kernel):
        return torch.abs(torch.sum(torch.clamp(kernel, min=-100, max=0)))